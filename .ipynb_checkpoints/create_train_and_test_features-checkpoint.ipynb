{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import itertools\n",
    "import re\n",
    "import string\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from collections import Counter\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spacy has some default rules for spliting text into sentences, as our text is already split\n",
    "# disabled this feature\n",
    "def prevent_sentence_boundary_detection(doc):\n",
    "    for token in doc:\n",
    "        # This will entirely disable spaCy's sentence detection\n",
    "        token.is_sent_start = False\n",
    "    return doc\n",
    "\n",
    "nlp.add_pipe(prevent_sentence_boundary_detection, name='prevent-sbd', before='parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_bad_splits(parsed):\n",
    "    \"\"\" Default tokenizer splits over some or all '-'s do this as adding rules wasn't working\"\"\"\n",
    "    for token in parsed:\n",
    "        if re.fullmatch(r'[A-Z]', token.text) is not None:\n",
    "            i = token.i\n",
    "            if i == 0:\n",
    "                continue\n",
    "            with parsed.retokenize() as retokenizer:\n",
    "                retokenizer.merge(parsed[i-1:i+1])\n",
    "            return join_bad_splits(parsed)\n",
    "        if token.text == '-':\n",
    "            i = token.i\n",
    "            with parsed.retokenize() as retokenizer:\n",
    "                retokenizer.merge(parsed[i-1:i+2])\n",
    "            # Merging removes a token, so iterating over the list goes out of index    \n",
    "            return join_bad_splits(parsed)\n",
    "    return parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shortest_path(graph, pairs):\n",
    "    \"\"\" Gets the shortest dependency tree paths for each pair\"\"\"\n",
    "    path_lens = []\n",
    "    for p in pairs:\n",
    "        try:\n",
    "            path_lens.append(nx.shortest_path_length(graph, p[0], p[1]))\n",
    "        except:\n",
    "            continue\n",
    "    if len(path_lens) == 0:\n",
    "        return [-1]\n",
    "    return path_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ner_term(ner, token):\n",
    "    \"\"\" Check if the ner term matches the token, if there is punctuation in the ner,\n",
    "        check if it is a substring of the token\"\"\"\n",
    "    subtokens = re.split(r'[\\.\\,\\+\\*/-]', token)\n",
    "    ner_split = re.split(r'[\\.\\,\\+\\*/-]', token)\n",
    "    if len(ner_split) != 1:\n",
    "        return ner in token\n",
    "    return ner == token or ner in subtokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_distance(gene, disease, parsed):\n",
    "    \"\"\" Get the minimum, maxium, and average minimal dep tree distance for the terms in a sentence\"\"\"\n",
    "    edges = []\n",
    "    gene_mentions = []\n",
    "    disease_mentions = []\n",
    "    for token in parsed:\n",
    "        token_format = '{0}-{1}'.format(token.text, token.i)\n",
    "        if find_ner_term(gene, token.text):\n",
    "            gene_mentions.append(token_format)\n",
    "        if find_ner_term(disease, token.text):\n",
    "            disease_mentions.append(token_format)\n",
    "        for child in token.children:\n",
    "            edges.append((token_format, '{0}-{1}'.format(child.text, child.i)))\n",
    "    graph = nx.Graph(edges)\n",
    "    pairs = [(g, d) for g in gene_mentions for d in disease_mentions]\n",
    "    min_dists = get_shortest_path(graph, pairs)\n",
    "    if len(min_dists) == 0:\n",
    "        min_dists = [-1]\n",
    "    word_dists = [abs(int(p[0].rsplit('-', 1)[1]) - int(p[1].rsplit('-', 1)[1])) for p in pairs]\n",
    "    try:\n",
    "        return [min_dists[0], word_dists[0]]  # Currently only 1 pair per sentence given tags\n",
    "    except:\n",
    "        print(gene, disease, [t.text for t in parsed], parsed[-1].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_ancestor(gene, disease, doc):\n",
    "    \"\"\" Finds the closest ancestor for gene/disease \"\"\"\n",
    "    gene_ancestors = []\n",
    "    dis_ancestors = []\n",
    "    # Get ancestors for each gene token\n",
    "    for token in doc:\n",
    "        if find_ner_term(gene, token.text):\n",
    "            # Need to reverse list an select the first before they are different\n",
    "            gene_ancestors.append([(a.text, a.i) for a in token.ancestors][::-1])\n",
    "        if find_ner_term(disease, token.text):\n",
    "            dis_ancestors.append([(a.text, a.i) for a in token.ancestors][::-1])\n",
    "    pairs = [(g,d) for g in gene_ancestors for d in dis_ancestors]\n",
    "    common_ancestors = []\n",
    "    for p in pairs:\n",
    "        common = ''\n",
    "        depth = -1\n",
    "        for gene_ancestor, disease_ancestor in zip(p[0], p[1]):\n",
    "            if gene_ancestor == disease_ancestor:\n",
    "                common = disease_ancestor[0]\n",
    "                depth += 1 \n",
    "            # if they are different the trees diverge\n",
    "            else:\n",
    "                break\n",
    "        common_ancestors.append((common, depth, len(p[0]) - depth, len(p[1]) - depth))\n",
    "    return common_ancestors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_dist(doc, counts):\n",
    "    \"\"\" Gives the normalized (sum of tags = 1) pos distribution\"\"\"\n",
    "    counter = {k:0 for k in list(counts.keys())}\n",
    "    for token in doc:\n",
    "        if token.pos_ in counter:\n",
    "            counter[token.pos_] += 1\n",
    "        else:\n",
    "            # The X POS tag is other, can be used if POS not present in main counts\n",
    "            counter['X'] += 1\n",
    "    # Normalize counts to sum to 1\n",
    "    return [x/len(doc) for x in list(counter.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_root_normalized(doc, counts):\n",
    "    \"\"\" Gives the normalized count of chunk value by # of chunks  for top 100 lemma roots of \n",
    "        chunks in training set\"\"\"\n",
    "    counter = {k:0 for k in [x[0] for x in counts.most_common(100)]}\n",
    "    n_chunks = 0\n",
    "    for chunk in doc.noun_chunks:\n",
    "        n_chunks += 1\n",
    "        if chunk.root.lemma_ in counter:\n",
    "            counter[chunk.root.lemma_] += 1\n",
    "    # Normalize counts\n",
    "    return [x/n_chunks for x in list(counter.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_head_normalized(doc, counts):\n",
    "    \"\"\" Gives the normalized count of chunk value by # of chunks  for top 100 lemma heads of \n",
    "        chunks in training set\"\"\"\n",
    "    counter = {k:0 for k in [x[0] for x in counts.most_common(100)]}\n",
    "    n_chunks = 0\n",
    "    for chunk in doc.noun_chunks:\n",
    "        n_chunks += 1\n",
    "        if chunk.root.head.lemma_ in counter:\n",
    "            counter[chunk.root.head.lemma_] += 1\n",
    "    # Normalize counts\n",
    "    return [x/n_chunks for x in list(counter.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negation_presence(doc):\n",
    "    \"\"\" Returns whether there are any negation dependencies in the document.\"\"\"\n",
    "    return any([token.dep_ == 'neg' for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ancestor_negation(gene, disease, doc):\n",
    "    \"\"\" Returns an int for whether an ancestor is negated in order from\n",
    "        root -> most common in parse tree. \"\"\"\n",
    "    gene_ancestors = []\n",
    "    dis_ancestors = []\n",
    "    # Get ancestors for each gene token\n",
    "    for token in doc:\n",
    "        if find_ner_term(gene, token.text):\n",
    "            # Need to reverse list an select the first before they are different\n",
    "            gene_ancestors.append([a.i for a in token.ancestors][::-1])\n",
    "        if find_ner_term(disease, token.text):\n",
    "            dis_ancestors.append([a.i for a in token.ancestors][::-1])\n",
    "    pairs = [(g,d) for g in gene_ancestors for d in dis_ancestors] # Should only be 1 pair in this set\n",
    "    common_ancestors = []\n",
    "    for p in pairs:\n",
    "        common = []\n",
    "        for gene_ancestor, disease_ancestor in zip(p[0], p[1]):\n",
    "            if gene_ancestor == disease_ancestor:\n",
    "                common.append(disease_ancestor)   \n",
    "            # if they are different the trees diverge\n",
    "            else:\n",
    "                break\n",
    "        common_ancestors += common\n",
    "    negations = []\n",
    "    for token in doc:\n",
    "        if token.dep_ == 'neg':\n",
    "            negations.append(token.head.i)\n",
    "    return int(any([c in negations for c in common_ancestors]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negated_ancestor_noun(doc):\n",
    "    \"\"\" Gives the indexes for the first ancestor of a negation word which is not a noun,\n",
    "        for all negations in a document\"\"\"\n",
    "    indexes = []\n",
    "    for token in doc:\n",
    "        if token.dep_ == 'neg':\n",
    "            head = token.head\n",
    "            while head.dep_ != 'ROOT':\n",
    "                if head.pos_ == 'NOUN':\n",
    "                    head = head.head\n",
    "                else:\n",
    "                    indexes.append(head.i)\n",
    "                    break\n",
    "            if head.dep_ == 'ROOT':\n",
    "                indexes.append(head.i)\n",
    "    return indexes\n",
    "\n",
    "def step_tree(term, doc):\n",
    "    \"\"\" Gives the token indexes for the ancestors of a given term\"\"\"\n",
    "    indexes = []\n",
    "    for token in doc:\n",
    "        if token.text == term:\n",
    "            head = token\n",
    "            while head.dep_ != 'ROOT':\n",
    "                head = head.head\n",
    "                indexes.append(head.i)\n",
    "    return indexes\n",
    "\n",
    "def noun_chunk_negated(gene, disease, doc):\n",
    "    \"\"\" Reuturns an int if one of the noun chunks in the paths to either the\n",
    "        disease or the gene are negated\"\"\"\n",
    "    shared = step_tree(gene, doc) + step_tree(disease, doc)\n",
    "    non_noun_neg = negated_ancestor_noun(doc)\n",
    "    return int(any([s in non_noun_neg for s in shared]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_draw = join_bad_splits(nlp(\"the results do not suggest a contribution of A2M and lrp to the development of MS.\"))\n",
    "html=spacy.displacy.render(doc_draw, style='dep', page=True, options={'distance': 100, 'compact':True})\n",
    "with open('/home/jsilva/winFiles/Downloads/output.html', 'w') as fp:\n",
    "    fp.write(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../dataset/final_train.csv', header=None, skiprows=[0],\n",
    "                  names=['id', 'assoc', 'gene', 'disease', 'sentence'])\n",
    "docs = []\n",
    "for index, entry in data.iterrows():\n",
    "    docs.append(join_bad_splits(nlp(entry.sentence)))\n",
    "    \n",
    "pos_counts = Counter()\n",
    "for doc in docs:\n",
    "    for token in doc:\n",
    "        pos_counts[token.pos_] += 1\n",
    "\n",
    "chunk_roots = Counter()\n",
    "for doc in docs:\n",
    "    for chunk in doc.noun_chunks:\n",
    "        chunk_roots[chunk.root.lemma_] += 1\n",
    "\n",
    "chunk_heads = Counter()\n",
    "for doc in docs:\n",
    "    for chunk in doc.noun_chunks:\n",
    "        chunk_heads[chunk.root.head.lemma_] += 1\n",
    "# This makes the feature data frame\n",
    "features = pd.DataFrame()\n",
    "tree_dists = []\n",
    "word_dists = []\n",
    "for e, d in zip(data.iterrows(), docs):\n",
    "    result = tree_distance(e[1].gene, e[1].disease, d)\n",
    "    tree_dists.append(result[0])\n",
    "    word_dists.append(result[1])\n",
    "features['tree_dists'] = tree_dists\n",
    "features['word_dists'] = word_dists\n",
    "common_words = []\n",
    "common_depth = []\n",
    "gene_fork_len = []\n",
    "disease_fork_len = []\n",
    "for d, e in zip(docs, data.iterrows()):\n",
    "    result = common_ancestor(e[1].gene, e[1].disease, d)[0]\n",
    "    common_words.append(result[0])\n",
    "    common_depth.append(result[1])\n",
    "    gene_fork_len.append(result[2])\n",
    "    disease_fork_len.append(result[3])\n",
    "features['common_word'] = common_words\n",
    "features['common_depth'] = common_depth\n",
    "features['gene_fork_len'] = gene_fork_len\n",
    "features['disease_fork_len'] = disease_fork_len\n",
    "any_negation = []\n",
    "for d in docs:\n",
    "    any_negation.append(int(negation_presence(d)))\n",
    "features['any_negation'] = any_negation\n",
    "shared_negation = []\n",
    "for d, e in zip(docs, data.iterrows()):\n",
    "    shared_negation.append(ancestor_negation(e[1].gene, e[1].disease, d))\n",
    "features['shared_negation'] = shared_negation   \n",
    "noun_negated = []\n",
    "for d, e in zip(docs, data.iterrows()):\n",
    "    noun_negated.append(noun_chunk_negated(e[1].gene, e[1].disease, d))\n",
    "features['noun_negated'] = noun_negated\n",
    "pos_cols = list(pos_counts.keys())\n",
    "pos_dist_results = []\n",
    "for doc in docs:\n",
    "        pos_dist_results.append(pos_dist(doc, pos_counts))\n",
    "pos_df = pd.DataFrame(pos_dist_results, columns=pos_cols)\n",
    "features = pd.merge(features, pos_df, left_index=True, right_index=True)\n",
    "chunk_root_cols = list(chunk_roots.keys())[:100]\n",
    "chunk_root_results = []\n",
    "for doc in docs:\n",
    "     chunk_root_results.append(chunk_root_normalized(doc, chunk_roots))\n",
    "root_df = pd.DataFrame(chunk_root_results, columns=chunk_root_cols)\n",
    "features = pd.merge(features, root_df, left_index=True, right_index=True)\n",
    "chunk_head_cols = list(chunk_heads.keys())[:100]\n",
    "chunk_head_results = []\n",
    "for doc in docs:\n",
    "    chunk_head_results.append(chunk_head_normalized(doc, chunk_heads))\n",
    "head_df = pd.DataFrame(chunk_head_results, columns=chunk_head_cols)\n",
    "features = pd.merge(features, head_df, left_index=True, right_index=True, suffixes=('_root', '_head'))\n",
    "features.to_csv(\"../corpus/features/left_features_train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Could make use of a function here\n",
    "data = pd.read_csv('../dataset/final_test.csv', header=None, skiprows=[0],\n",
    "                  names=['id', 'assoc', 'gene', 'disease', 'sentence'])\n",
    "docs = []\n",
    "for index, entry in data.iterrows():\n",
    "    docs.append(join_bad_splits(nlp(entry.sentence)))\n",
    "# We are reusing the top 100 counts from train for chunk root/pos/head for consistency of features\n",
    "# This makes the feature data frame\n",
    "features = pd.DataFrame()\n",
    "tree_dists = []\n",
    "word_dists = []\n",
    "for e, d in zip(data.iterrows(), docs):\n",
    "    result = tree_distance(e[1].gene, e[1].disease, d)\n",
    "    tree_dists.append(result[0])\n",
    "    word_dists.append(result[1])\n",
    "features['tree_dists'] = tree_dists\n",
    "features['word_dists'] = word_dists\n",
    "common_words = []\n",
    "common_depth = []\n",
    "gene_fork_len = []\n",
    "disease_fork_len = []\n",
    "for d, e in zip(docs, data.iterrows()):\n",
    "    result = common_ancestor(e[1].gene, e[1].disease, d)[0]\n",
    "    common_words.append(result[0])\n",
    "    common_depth.append(result[1])\n",
    "    gene_fork_len.append(result[2])\n",
    "    disease_fork_len.append(result[3])\n",
    "features['common_word'] = common_words\n",
    "features['common_depth'] = common_depth\n",
    "features['gene_fork_len'] = gene_fork_len\n",
    "features['disease_fork_len'] = disease_fork_len\n",
    "any_negation = []\n",
    "for d in docs:\n",
    "    any_negation.append(int(negation_presence(d)))\n",
    "features['any_negation'] = any_negation\n",
    "shared_negation = []\n",
    "for d, e in zip(docs, data.iterrows()):\n",
    "    shared_negation.append(ancestor_negation(e[1].gene, e[1].disease, d))\n",
    "features['shared_negation'] = shared_negation   \n",
    "noun_negated = []\n",
    "for d, e in zip(docs, data.iterrows()):\n",
    "    noun_negated.append(noun_chunk_negated(e[1].gene, e[1].disease, d))\n",
    "features['noun_negated'] = noun_negated\n",
    "pos_cols = list(pos_counts.keys())\n",
    "pos_dist_results = []\n",
    "for doc in docs:\n",
    "        pos_dist_results.append(pos_dist(doc, pos_counts))\n",
    "pos_df = pd.DataFrame(pos_dist_results, columns=pos_cols)\n",
    "features = pd.merge(features, pos_df, left_index=True, right_index=True)\n",
    "chunk_root_cols = list(chunk_roots.keys())[:100]\n",
    "chunk_root_results = []\n",
    "for doc in docs:\n",
    "     chunk_root_results.append(chunk_root_normalized(doc, chunk_roots))\n",
    "root_df = pd.DataFrame(chunk_root_results, columns=chunk_root_cols)\n",
    "features = pd.merge(features, root_df, left_index=True, right_index=True)\n",
    "chunk_head_cols = list(chunk_heads.keys())[:100]\n",
    "chunk_head_results = []\n",
    "for doc in docs:\n",
    "        chunk_head_results.append(chunk_head_normalized(doc, chunk_heads))\n",
    "head_df = pd.DataFrame(chunk_head_results, columns=chunk_head_cols)\n",
    "features = pd.merge(features, head_df, left_index=True, right_index=True, suffixes=('_root', '_head'))\n",
    "features.to_csv(\"../corpus/features/left_features_test.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
