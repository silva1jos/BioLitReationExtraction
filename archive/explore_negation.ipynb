{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import itertools\n",
    "import re\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from collections import Counter\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spacy has some default rules for spliting text into sentences, as our text is already split\n",
    "# disabled this feature\n",
    "def prevent_sentence_boundary_detection(doc):\n",
    "    for token in doc:\n",
    "        # This will entirely disable spaCy's sentence detection\n",
    "        token.is_sent_start = False\n",
    "    return doc\n",
    "\n",
    "nlp.add_pipe(prevent_sentence_boundary_detection, name='prevent-sbd', before='parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_bad_splits(parsed):\n",
    "    \"\"\" Default tokenizer splits over some or all '-'s do this as adding rules wasn't working\"\"\"\n",
    "    for token in parsed:\n",
    "        if re.fullmatch(r'[A-Z]', token.text) is not None:\n",
    "            i = token.i\n",
    "            if i == 0:\n",
    "                continue\n",
    "            with parsed.retokenize() as retokenizer:\n",
    "                retokenizer.merge(parsed[i-1:i+1])\n",
    "            return join_bad_splits(parsed)\n",
    "        if token.text == '-':\n",
    "            i = token.i\n",
    "            with parsed.retokenize() as retokenizer:\n",
    "                retokenizer.merge(parsed[i-1:i+2])\n",
    "            # Merging removes a token, so iterating over the list goes out of index    \n",
    "            return join_bad_splits(parsed)\n",
    "    return parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ner_term(ner, token):\n",
    "    \"\"\" Check if the ner term matches the token, if there is punctuation in the ner,\n",
    "        check if it is a substring of the token\"\"\"\n",
    "    subtokens = re.split(r'[\\.\\,\\+\\*/-]', token)\n",
    "    ner_split = re.split(r'[\\.\\,\\+\\*/-]', token)\n",
    "    if len(ner_split) != 1:\n",
    "        return ner in token\n",
    "    return ner == token or ner in subtokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ancestor_negation(gene, disease, doc):\n",
    "    \"\"\" Returns a list of booleans for whether each ancestor is negated in order from\n",
    "        root -> most common in parse tree. Returns an int\"\"\"\n",
    "    gene_ancestors = []\n",
    "    dis_ancestors = []\n",
    "    # Get ancestors for each gene token\n",
    "    for token in doc:\n",
    "        if find_ner_term(gene, token.text):\n",
    "            # Need to reverse list an select the first before they are different\n",
    "            gene_ancestors.append([a.i for a in token.ancestors][::-1])\n",
    "        if find_ner_term(disease, token.text):\n",
    "            dis_ancestors.append([a.i for a in token.ancestors][::-1])\n",
    "    pairs = [(g,d) for g in gene_ancestors for d in dis_ancestors]\n",
    "    common_ancestors = []\n",
    "    for p in pairs:\n",
    "        common = []\n",
    "        for gene_ancestor, disease_ancestor in zip(p[0], p[1]):\n",
    "            if gene_ancestor == disease_ancestor:\n",
    "                common.append(disease_ancestor)   \n",
    "            # if they are different the trees diverge\n",
    "            else:\n",
    "                break\n",
    "        common_ancestors += common\n",
    "    common_ancestors = set(common_ancestors) # In case there are multiple pairs, shouldn't be anymore\n",
    "    negations = []\n",
    "    for token in doc:\n",
    "        if token.dep_ == 'neg':\n",
    "            negations.append(token.head.i)\n",
    "    return [(c in negations) for c in common_ancestors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ner(x):\n",
    "    return x.upper().replace(' ', '_')\n",
    "\n",
    "data = pd.read_csv('../dataset/GAD_Y_N_wPubmedID_annotated_cap.csv', usecols=[2, 6, 9, 11], skiprows = [0],\n",
    "                   header=None, names=['rel', 'gene', 'disease', 'sentence'])\n",
    "\n",
    "data.gene = data.gene.apply(process_ner)\n",
    "data.disease = data.disease.apply(process_ner)\n",
    "data.rel = data.rel.apply(lambda x: x == 'Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "for index, entry in data.iterrows():\n",
    "    docs.append(join_bad_splits(nlp(entry.sentence)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "604 63\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'neg'  present, detect 378 with most common word, detect 465 other wise\n",
    "i = 0\n",
    "j = 0\n",
    "for d, e in zip(docs, data.iterrows()):\n",
    "    ancestors = ancestor_negation(e[1].gene, e[1].disease, d)\n",
    "    shared = step_tree(e[1].gene, d) + step_tree(e[1].disease, d)\n",
    "    non_noun_neg = negated_ancestor_noun(d)\n",
    "    noun_approach = [s in non_noun_neg for s in shared]\n",
    "    if any([token.dep_ == 'neg' for token in d]):\n",
    "        i += 1\n",
    "        if any(ancestors) != any(noun_approach):\n",
    "            #print(any(ancestors), any(noun_approach), e[1].sentence, '\\n')\n",
    "            j += 1\n",
    "print(i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negated_root(doc):\n",
    "    \"\"\" Returns whether the root node is negated in the document.\"\"\"\n",
    "    flag = False\n",
    "    for token in doc:    \n",
    "        if token.dep_ == 'neg' and token.head.dep_ == 'ROOT':\n",
    "            flag = not flag # Need to apply each itme the root is negated\n",
    "    return flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negation_presence(doc):\n",
    "    \"\"\" Returns whether there are any negation dependencies in the document.\"\"\"\n",
    "    return any([token.dep_ == 'neg' for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negated_noun_chunk(term, doc):\n",
    "    \"\"\" Returns wehther the noun chunk is negated. \"\"\"\n",
    "    for chunk in doc.noun_chunks:\n",
    "        print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = nlp('based on these data, ABCB6 is not the causative gene for GRACILE_SYNDROME.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negated_ancestor_noun(doc):\n",
    "    \"\"\" Gives the indexes for the first ancestor of a negation word which is not a noun,\n",
    "        for all negations in a document\"\"\"\n",
    "    indexes = []\n",
    "    for token in doc:\n",
    "        if token.dep_ == 'neg':\n",
    "            head = token.head\n",
    "            while head.dep_ != 'ROOT':\n",
    "                if head.pos_ == 'NOUN':\n",
    "                    head = head.head\n",
    "                else:\n",
    "                    indexes.append(head.i)\n",
    "                    break\n",
    "            if head.dep_ == 'ROOT':\n",
    "                indexes.append(head.i)\n",
    "    return indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negated_ancestor_noun(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_tree(term, doc):\n",
    "    \"\"\" Gives the token indexes for the ancestors of a given term\"\"\"\n",
    "    indexes = []\n",
    "    for token in doc:\n",
    "        if token.text == term:\n",
    "            head = token\n",
    "            while head.dep_ != 'ROOT':\n",
    "                head = head.head\n",
    "                indexes.append(head.i)\n",
    "    return indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negated_ancestor_noun(test) in step_tree('ACE', test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14, 4, 18, 2]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step_tree('ACE', test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
