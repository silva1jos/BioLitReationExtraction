{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import itertools\n",
    "import re\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from collections import Counter\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spacy has some default rules for spliting text into sentences, as our text is already split\n",
    "# disabled this feature\n",
    "def prevent_sentence_boundary_detection(doc):\n",
    "    for token in doc:\n",
    "        # This will entirely disable spaCy's sentence detection\n",
    "        token.is_sent_start = False\n",
    "    return doc\n",
    "\n",
    "nlp.add_pipe(prevent_sentence_boundary_detection, name='prevent-sbd', before='parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_bad_splits(parsed):\n",
    "    \"\"\" Default tokenizer splits over some or all '-'s do this as adding rules wasn't working\"\"\"\n",
    "    for token in parsed:\n",
    "        if re.fullmatch(r'[A-Z]', token.text) is not None:\n",
    "            i = token.i\n",
    "            if i == 0:\n",
    "                continue\n",
    "            with parsed.retokenize() as retokenizer:\n",
    "                retokenizer.merge(parsed[i-1:i+1])\n",
    "            return join_bad_splits(parsed)\n",
    "        if token.text == '-':\n",
    "            i = token.i\n",
    "            with parsed.retokenize() as retokenizer:\n",
    "                retokenizer.merge(parsed[i-1:i+2])\n",
    "            # Merging removes a token, so iterating over the list goes out of index    \n",
    "            return join_bad_splits(parsed)\n",
    "    return parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shortest_path(graph, pairs):\n",
    "    \"\"\" Gets the shortest dependency tree paths for each pair\"\"\"\n",
    "    path_lens = []\n",
    "    for p in pairs:\n",
    "        try:\n",
    "            path_lens.append(nx.shortest_path_length(graph, p[0], p[1]))\n",
    "        except:\n",
    "            continue\n",
    "    if len(path_lens) == 0:\n",
    "        return [-1]\n",
    "    return path_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ner_term(ner, token):\n",
    "    \"\"\" Check if the ner term matches the token, if there is punctuation in the ner,\n",
    "        check if it is a substring of the token\"\"\"\n",
    "    subtokens = re.split(r'[\\.\\,\\+\\*/-]', token)\n",
    "    ner_split = re.split(r'[\\.\\,\\+\\*/-]', token)\n",
    "    if len(ner_split) != 1:\n",
    "        return ner in token\n",
    "    return ner == token or ner in subtokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_distance(gene, disease, parsed):\n",
    "    \"\"\" Get the minimum, maxium, and average minimal dep tree distance for the terms in a sentence\"\"\"\n",
    "    edges = []\n",
    "    gene_mentions = []\n",
    "    disease_mentions = []\n",
    "    for token in parsed:\n",
    "        token_format = '{0}-{1}'.format(token.text, token.i)\n",
    "        if find_ner_term(gene, token.text):\n",
    "            gene_mentions.append(token_format)\n",
    "        if find_ner_term(disease, token.text):\n",
    "            disease_mentions.append(token_format)\n",
    "        for child in token.children:\n",
    "            edges.append((token_format, '{0}-{1}'.format(child.text, child.i)))\n",
    "    graph = nx.Graph(edges)\n",
    "    pairs = [(g, d) for g in gene_mentions for d in disease_mentions]\n",
    "    min_dists = get_shortest_path(graph, pairs)\n",
    "    if len(min_dists) == 0:\n",
    "        min_dists = [-1]\n",
    "    word_dists = [abs(int(p[0].rsplit('-', 1)[1]) - int(p[1].rsplit('-', 1)[1])) for p in pairs]\n",
    "    try:\n",
    "        return [min_dists[0], word_dists[0]]  # Currently only 1 pair per sentence given tags\n",
    "    except:\n",
    "        print(gene, disease, [t.text for t in parsed])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_ancestor(gene, disease, doc):\n",
    "    \"\"\" Finds the closest ancestor for gene/disease \"\"\"\n",
    "    gene_ancestors = []\n",
    "    dis_ancestors = []\n",
    "    # Get ancestors for each gene token\n",
    "    for token in doc:\n",
    "        if find_ner_term(gene, token.text):\n",
    "            # Need to reverse list an select the first before they are different\n",
    "            gene_ancestors.append([(a.text, a.i) for a in token.ancestors][::-1])\n",
    "        if find_ner_term(disease, token.text):\n",
    "            dis_ancestors.append([(a.text, a.i) for a in token.ancestors][::-1])\n",
    "    pairs = [(g,d) for g in gene_ancestors for d in dis_ancestors]\n",
    "    common_ancestors = []\n",
    "    for p in pairs:\n",
    "        common = ''\n",
    "        depth = -1\n",
    "        for gene_ancestor, disease_ancestor in zip(p[0], p[1]):\n",
    "            if gene_ancestor == disease_ancestor:\n",
    "                common = disease_ancestor[0]\n",
    "                depth += 1 \n",
    "                \n",
    "            # if they aare different the trees diverge\n",
    "            else:\n",
    "                break\n",
    "        common_ancestors.append((common, depth, len(p[0]) - depth, len(p[1]) - depth))\n",
    "    return set(common_ancestors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_counts = Counter()\n",
    "for doc in docs:\n",
    "    for token in doc:\n",
    "        pos_counts[token.pos_] += 1\n",
    "          \n",
    "def pos_dist(doc):\n",
    "    \"\"\" Gives the normalized (sum of tags = 1) pos distribution\"\"\"\n",
    "    counter = {k:0 for k in list(pos_counts.keys())}\n",
    "    for token in doc:\n",
    "        if token.pos_ in counter:\n",
    "            counter[token.pos_] += 1\n",
    "        else:\n",
    "            # The X POS tag is other, can be used if POS not present in main counts\n",
    "            counter['X'] += 1\n",
    "    # Normalize counts to sum to 1\n",
    "    return [x/len(doc) for x in list(counter.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_roots = Counter()\n",
    "for doc in docs:\n",
    "    for chunk in doc.noun_chunks:\n",
    "        chunk_roots[chunk.root.lemma_] += 1\n",
    "          \n",
    "def chunk_root_normalized(doc):\n",
    "    \"\"\" Gives the normalized count of chunk value by # of chunks  for top 100 lemma roots of \n",
    "        chunks in training set\"\"\"\n",
    "    counter = {k:0 for k in [x[0] for x in chunk_roots.most_common(100)]}\n",
    "    n_chunks = 0\n",
    "    for chunk in doc.noun_chunks:\n",
    "        n_chunks += 1\n",
    "        if chunk.root.lemma_ in counter:\n",
    "            counter[chunk.root.lemma_] += 1\n",
    "    # Normalize counts\n",
    "    return [x/n_chunks for x in list(counter.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_heads = Counter()\n",
    "for doc in docs:\n",
    "    for chunk in doc.noun_chunks:\n",
    "        chunk_heads[chunk.root.head.lemma_] += 1\n",
    "          \n",
    "def chunk_head_normalized(doc):\n",
    "    \"\"\" Gives the normalized count of chunk value by # of chunks  for top 100 lemma heads of \n",
    "        chunks in training set\"\"\"\n",
    "    counter = {k:0 for k in [x[0] for x in chunk_heads.most_common(100)]}\n",
    "    n_chunks = 0\n",
    "    for chunk in doc.noun_chunks:\n",
    "        n_chunks += 1\n",
    "        if chunk.root.lemma_ in counter:\n",
    "            counter[chunk.root.lemma_] += 1\n",
    "    # Normalize counts\n",
    "    return [x/n_chunks for x in list(counter.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ner(x):\n",
    "    return x.upper().replace(' ', '_')\n",
    "\n",
    "data = pd.read_csv('../dataset/GAD_Y_N_wPubmedID_annotated_cap.csv', usecols=[6, 9, 11], skiprows = [0],\n",
    "                   header=None, names=['gene', 'disease', 'sentence'])\n",
    "\n",
    "data.gene = data.gene.apply(process_ner)\n",
    "data.disease = data.disease.apply(process_ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "for index, entry in data.iterrows():\n",
    "    docs.append(join_bad_splits(nlp(entry.sentence)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('annotated_cap_dist_features.csv', 'w') as f:\n",
    "    writer = csv.writer(f, lineterminator='\\n')\n",
    "    for e, d in zip(data.iterrows(), docs):\n",
    "        writer.writerow(tree_distance(e[1].gene, e[1].disease, d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('annotated_cap_common_word.csv', 'w') as f:\n",
    "    writer = csv.writer(f, lineterminator='\\n')   \n",
    "    for d, e in zip(docs, data.iterrows()):\n",
    "        writer.writerow(common_ancestor(e[1].gene, e[1].disease, d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('annotated_cap_pos_dist.csv', 'w') as f:\n",
    "    writer = csv.writer(f, lineterminator='\\n')\n",
    "    writer.writerow(list(pos_counts.keys()))\n",
    "    for doc in docs:\n",
    "        writer.writerow(pos_dist(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('annotated_cap_chunk_roots.csv', 'w') as f:\n",
    "    writer = csv.writer(f, lineterminator='\\n')\n",
    "    writer.writerow(list(chunk_roots.keys()))\n",
    "    for doc in docs:\n",
    "        writer.writerow(chunk_root_normalized(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('annotated_cap_chunk_heads.csv', 'w') as f:\n",
    "    writer = csv.writer(f, lineterminator='\\n')\n",
    "    writer.writerow(list(chunk_heads.keys()))\n",
    "    for doc in docs:\n",
    "        writer.writerow(chunk_head_normalized(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d, e in zip(docs, data.iterrows()):\n",
    "    if 1 != sum([find_ner_term(e[1].gene, t.text) for t in d]):\n",
    "        print(e[1].gene, e[1].sentence)\n",
    "    if 1 != sum([find_ner_term(e[1].disease, t.text) for t in d]):\n",
    "        print(e[1].disease, e[1].sentence)\n",
    "        test = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[our,\n",
       " results,\n",
       " suggest,\n",
       " that,\n",
       " CTLA-4,\n",
       " gene,\n",
       " polymorphisms,\n",
       " may,\n",
       " partially,\n",
       " be,\n",
       " involved,\n",
       " in,\n",
       " the,\n",
       " susceptibility,\n",
       " to,\n",
       " CHRONIC_HEPATITIS_B.]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[t for t in test]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
